# 图像分割(含语义/实例/全景分割)

## (1) 对图像分割的基本认识

### 1. 什么是超像素、语义分割、实例分割、全景分割？

图像分割（image segmentation）任务的定义是：根据某些规则将图片分成若干个特定的、具有独特性质的区域，并提出感兴趣目标的技术和过程。

目前图像分割任务发展出了以下几个子领域：语义分割（semantic segmentation）、实例分割（instance segmentation）以及今年刚兴起的新领域全景分割（panoptic segmentation）。

而想要理清三个子领域的区别就不得不提到关于图像分割中 things 和 stuff 的区别：图像中的内容可以按照是否有固定形状分为 things 类别和 stuff 类别，其中，人，车等有固定形状的物体属于 things 类别（可数名词通常属于 things）；天空，草地等没有固定形状的物体属于 stuff 类别（不可数名词属于 stuff）。

语义分割更注重「类别之间的区分」，而实例分割更注重「个体之间的区分」，以下图为例，从上到下分别是原图、语义分割结果和实例分割结果。语义分割会重点将前景里的人群和背景里树木、天空和草地分割开，但是它不区分人群的单独个体，如图中的人全部标记为红色，导致右边黄色框中的人无法辨别是一个人还是不同的人；而实例分割会重点将人群里的每一个人分割开，但是不在乎草地、树木和天空的分割。

- [超像素、语义分割、实例分割、全景分割 傻傻分不清？ - 知乎](https://zhuanlan.zhihu.com/p/50996404)
- [全景分割这一年，端到端之路 | 机器之心](https://www.jiqizhixin.com/articles/2018-12-24-12)



### 2. 什么是同物异谱、同谱异物？

同物异谱：在相同的地物上，由于周围环境、病虫害或者放射性物质等影响，造成的相同的物种但是其光谱曲线不同。

异物同谱：在某一谱段区，两个不同地物可能会呈现相同的谱线特征。这是同谱异物，也可能是同一个地物，处于不同的状态，如对太阳光相对角度不同，密度不同，含水量不同，生长环境影响光谱曲线，呈现不同的谱线特征。

这两种现象在主要依靠光谱信息进行分类的算法上影响是很大，很容易造成错分、误分，在 3D-CNN 上，利用空谱联合信息来处理高光谱影像，在某种程度上可以削弱同物异谱或者同谱异物的影响。

也给遥感分类造成了困难，遥感影像在分类时主要依靠的就是地物的光谱特征，尤其是非监督分类，它的前提就是不存在“同物异谱”和异物同谱“现象。

*参考：[高光谱中的同物异谱及同谱异物现象](https://blog.csdn.net/u012193416/article/details/80683929)*



### 3. RGB图像、全色图像、多光谱图像、高光谱图像？

*参考：[详细理解RGB图像、全色图像、多光谱图像、高光谱图像  - CSDN博客](https://blog.csdn.net/Chaolei3/article/details/79404806)*



### 4. 什么是光学图像？什么是SAR图像？它们的区别是什么？成像机制有什么差异？在图像分割上有什么不同？

SAR 是主动式侧视雷达系统，且成像几何属于斜距投影类型。因此 SAR 图像与光学图像在成像机理、几何特征、辐射特征等方面都有较大的区别。在进行 SAR 图像处理和应用前，需要了解 SAR 图像的基本特征。

*——from：[SAR图像简介 - xuanzi_eli的博客 - CSDN博客](https://blog.csdn.net/xuanzi_eli/article/details/52334789)*

按传感器采用的成像波段分类，光学图像通常是指可见光和部分红外波段传感器获取的影像数据。而 SAR 传感器基本属于微波频段，波长通常在厘米级。

可见光图像通常会包含多个波段的灰度信息，以便于识别目标和分类提取。而 SAR 图像则只记录了一个波段的回波信息，以二进制复数形式记录下来；但基于每个像素的复数数据可变换提取相应的振幅和相位信息。振幅信息通常对应于地面目标对雷达波的后向散射强度，与目标介质、含水量以及粗糙程度密切相关；该信息与可见光成像获得的灰度信息有较大的相关性。而相位信息则对应于传感器平台与地面目标的往返传播距离，这与 GPS 相位测距的原理相同。

由于 SAR 影像分辨率相对较低、信噪比较低，所以 SAR 影像中所包含的振幅信息远达不到同光学影像的成像水平；但其特有的相位信息是其他传感器所无法获取的，基于相位的干涉建模也是 SAR 的主要应用方向。

在成像模式方面，光学影像通常采用中心投影面域成像或推帚式扫描获取数据；而 SAR 处于信号处理的需要（合成孔径过程，这里就不展开讨论了）不能采用垂直向下的照射方式而只能通过测视主动成像方式发射和接受面域雷达波，并通过信号处理（聚焦、压缩、滤波等）手段后期合成对应于地面目标的复数像元。

单一 SAR 影像的相位信息基本没有统计特征，只有振幅信息可用于目标识别和分类等应用。正如前面所说，振幅信息深受噪声的影响，加之 SAR 影像特有的几何畸变（叠掩、透视收缩、多路径虚假目标等）特征，个人认为仁兄若是想在图像分割领域做探讨的话，可以直接忽略掉 SAR 影像了。

*——form：[什么是光学图像？什么是SAR图像？它们的区别是什么？成像机制有什么差异？在图像分割上有什么不同？_百度知道](https://zhidao.baidu.com/question/372993201.html)*



### 5. 遥感图像处理和普通图像处理有哪些异同呢，本质区别是什么？

*参考：[遥感图像处理和普通图像处理有哪些异同呢，本质区别是什么？ - 知乎](https://www.zhihu.com/question/29738706)*



### 6. 什么是超分辨率？

超分辨率(Super-Resolution)即通过硬件或软件的方法提高原有图像的分辨率，通过一系列低分辨率的图像来得到一幅高分辨率的图像过程就是超分辨率重建。超分辨率重建的核心思想就是用时间带宽(获取同一场景的多帧图像序列)换取空间分辨率,实现时间分辨率向空间分辨率的转换。*——from：[超分辨率_百度百科](https://baike.baidu.com/item/%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87)* 

超分辨率技术（Super-Resolution）是指从观测到的低分辨率图像重建出相应的高分辨率图像，在监控设备、卫星图像和医学影像等领域都有重要的应用价值。SR可分为两类:从多张低分辨率图像重建出高分辨率图像和从单张低分辨率图像重建出高分辨率图像。基于深度学习的SR，主要是基于单张低分辨率的重建方法，即Single Image Super-Resolution (SISR)。*——from：[深度学习在图像超分辨率重建中的应用 - 知乎](https://zhuanlan.zhihu.com/p/25532538)* 



### 7. 什么是高分辨遥感？

空间分辨率能够被传感器辨识的单一地物或 2 个相邻地物间的最小尺寸。空间分辨率越高，遥感图像包含的地物形态信息就越丰富，能识别的目标就越小。

高空间分辨率图像 简称“高分图像”，包含了地物丰富的纹理、形状、结构、邻域关系等信息，可主要应用于地物分类、目标提取与识别、变化检测等。——from：[不懂这些怎么好意思谈遥感 - 知乎](https://zhuanlan.zhihu.com/p/28279432)



### 8. 遥感图像分辨率？

遥感卫星的飞行高度一般在400km～600km之间，图像分辨率一般从1 km～1m之间。图像分辨率是什么意思呢？可以这样理解，一个像元，代表地面的面积是多少。像元是什么意思呢？像元相当于电视屏幕上的一个点（电视是由若干个点组成的图像画面），相当于计算机显示屏幕上的一个象素，相当于一群举着不同色板拼成画图的人中的一个。

**当分辨率为1km时，一个像元代表地面1kmX1km的面积，即1km^2；当分辨率为30m时，一个像元代表地面30m×30m的面积；当分辨率为1m时，也就是说，图像上的一个像元相当于地面1m x 1m的面积，即1m^2。**

——from：[影像分辨率_百度百科](https://baike.baidu.com/item/%E5%BD%B1%E5%83%8F%E5%88%86%E8%BE%A8%E7%8E%87/9143316)

PS：**亚米级高分辨率卫星影像**指的是能达到 1 米以下分辨率的卫星影像。国内亚米级高分辨卫星影像：高分二号卫星，全色影像分辨率0.8m。



### 9. 什么是掩膜（Mask）？



### 10. 什么是 heatmap？

对原图不断取卷积，降低尺寸大小直到最小的feature map，我们称之为 heatmap... ——from：https://zhuanlan.zhihu.com/p/47579399

### 11. 高斯噪声、胡椒噪声、盐噪声和椒盐噪声？

**高斯噪声：**

高斯噪声，顾名思义是指服从高斯分布（正态分布）的一类噪声，通常是因为**不良照明和高温**引起的传感器噪声。通常在RGB图像中，显现比较明显。如下图。

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190513213400.png)

**椒盐噪声：**

椒盐噪声，通常是由图像传感器，传输信道，解压处理等产生的黑白相间的亮暗点噪声（椒-黑，盐-白）。通常出现在灰度图中。如下图所示：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190513213410.png)

——from：https://blog.csdn.net/firstlai/article/details/77675344


图像加入椒盐噪声开始，椒盐噪声其实就是**使图像的一些随机的像素为黑色（0）或者白色（255）**：

- **盐噪声**又称**白噪声**，在图像中添加一些随机的白色像素点（255）；
- **胡椒噪声**是在图像中添加一些随机的黑色像素点（0）；
- **盐椒噪声**是在图像中既有白色像素点，又有黑色像素点。

### 12. 什么是 Alpha 通道？

**Alpha 通道是计算机图形学中的术语，指的是特别的通道，意思是“非彩色”通道，主要是用来保存选区和编辑选区。**

 [为什么用‘Alpha’代表透明度？ - 知乎](http://www.zhihu.com/question/21422818)

Alpha 没有透明度的意思，不代表透明度。opacity 和 transparency 才和透明度有关，前者是不透明度，后者是透明度。比如 css 中的「opacity: 0.5」就是设定元素有 50% 的不透明度。

一个图像的每个像素都有 RGB 三个通道，后来 [Alvy Ray Smith](http://en.wikipedia.org/wiki/Alvy_Ray_Smith) 提出每个像素再增加一个 Alpha 通道，取值为 0 到 1，用来储存这个像素是否对图片有「贡献」，0 代表透明、1 代表不透明。也就是说，「Alpha 通道」储存一个值，其外在表现是「透明度」，Alpha 和透明度没啥关系。

为什么取名为 Alpha 通道，我觉得是因为这是除 RGB 以外「第一个通道」的意思，没有别的更深刻的含义。

「Alpha 通道」是图片内在的一个属性，用 css 或者其他外部方法设定透明度，并没有改变图片的 Alpha 通道的值。

阿尔法通道（α Channel或Alpha Channel）是指一张图片的透明和半透明度。例如：一个使用每个像素16比特存储的位图，对于图形中的每一个像素而言，可能以5个比特表示红色，5个比特表示绿色，5个比特表示蓝色，最后一个比特是阿尔法。在这种情况下，它要么表示透明要么不是，因为阿尔法比特只有 0 或 1 两种不同表示的可能性。又如一个使用 32 个比特存储的位图，每 8 个比特表示红绿蓝，和阿尔法通道。在这种情况下，就不光可以表示透明还是不透明，阿尔法通道还可以表示 256 级的半透明度，因为阿尔法通道有 8 个比特可以有 256 种不同的数据表示可能性。

GBA 是代表 Red（红色） Green（绿色） Blue（蓝色）和 Alpha 的色彩空间。虽然它有的时候被描述为一个颜色空间，但是**它其实仅仅是 RGB 模型的附加了额外的信息。采用的颜色是 RGB**，可以属于任何一种 RGB 颜色空间，但是 Catmull 和 Smith 在1971至1972年间提出了这个不可或缺的 alpha 数值，使得 alpha 渲染和 alpha 合成变得可能。提出者以 alpha 来命名是源于经典的**线性插值方程αA + (1-α)B**所用的就是这个希腊字母。

**线性插值是数学、计算机图形学等领域广泛使用的一种简单插值方法。**

**真正让图片变透明的不是 Alpha 实际是 Alpha 所代表的数值和其他数值做了一次运算 。**

比如你有一张图片你想抠出图片中间的一部分在 PS 里你会建立一个蒙板，然后在蒙板里把不需要的地方填充成黑色，需要的留成白色，这个时候实际上是是做了一次乘法。

用黑色所代表的数值 0 去乘以你所填充的地方，那么这个地方就变透明了。

——from：[一个也许很傻的问题,在图像处理中alpha到底是什么？](http://bbs.csdn.net/topics/60421021)

在图像处理中，Alpha 用来衡量一个像素或图像的透明度。在非压缩的 32 位 RGB 图像中，每个像素是由四个部分组成：一个 Alpha 通道和三个颜色分量(R、G和B)。当 Alpha 值为 0 时，该像素是完全透明的，而当 Alpha 值为255时，则该像素是完全不透明。 

Alpha混色是将源像素和背景像素的颜色进行混合，最终显示的颜色取决于其RGB颜色分量和 Alpha 值。它们之间的关系可用下列公式来表示： 

`显示颜色 = 源像素颜色 X alpha / 255 + 背景颜色 X (255 - alpha) / 255`


它是通用图像格式 RGB 外的另外一种数据，用于表示除三原色之外像素点的透明度，分为 255 个透明度级别。

Alpha 是出现在 32 位位图文件中的一类数据，用于向图像中的像素指定透明度。24 位真彩文件包含三种颜色信息通道：红、绿和蓝或 RGB。每个通道在各个像素上都拥有具体的强度或值。每个通道的强度决定图像中像素的颜色。

通过添加第四种 alpha 通道，文件可以指定每个像素的透明度或不透明度。alpha 的值为 0 表示透明，alpha 的值为 255 则表示不透明，在此范围之间的值表示半透明。透明度对于合成操作是至关重要的，如在 Video Post 中，位于各个层中的几个图像要混合在一起。——from：[四通道图像中的alpha通道](<https://blog.csdn.net/H2008066215019910120/article/details/13558597>)

### 13. 遥感影像图中真彩色，假彩色，伪彩色是什么？

回答1：

> 就计算机显示的 RGB 三个分量，如果加载真实的红、绿、蓝波段，就是真彩色；
>
> 如果将其中的某一个或者几个波段替换成其他波段，就是假彩色，比如把近红外波段替换绿波段；
>
> 全色图像中将不同灰度按照不同等级赋予特定的颜色就是伪彩色，这是因为人眼对颜色的敏感程度远大于灰度。
>
> 后两个都是为了增强目视判读效果的。

回答2：

> 上面的解释很清楚了，定义我就不再说。
>
> 真彩色反映的真实的天然颜色。假彩色合成主要是为了用肉眼观察可以让目标显示得更突出。伪彩色显示主要是用颜色表达地物变化的情况，通常会把想要表达的地物特征都会投影到一个连续的颜色区间上。

——from：[遥感影像图中真彩色，假彩色，伪彩色是怎样定义的？为什么要分这些类？ - 知乎](<https://www.zhihu.com/question/37163449>)

回答3：

> 因为不同的波段合成的假彩色影像可以突出不同的地物。比如最常见的tm影像的假彩色合成作用如下。
>
> ——from：[为什么要做假彩色？ - 名字太生僻不写了的回答 - 知乎](https://www.zhihu.com/question/60794267/answer/180981942)

相关阅读：[数字图像的类型——伪彩色，真彩色，假彩色](<https://blog.csdn.net/chaolei3/article/details/79672162>)

> 假彩色图像：
>
> 假彩色图像也是 3 通道的，但是它的 3 个通道不再是 RGB 3 个波段的信息，而是用其他的波段来组成的 3 通道图像。如 landsat 7/ETM+有八个波段，用其中三个波段合成的图像就是假彩色图像。从实现技术上讲，假彩色与真彩色是一致的，都是 R、G、B 分量组合显示而伪彩色显示调用的是颜色表。

### 14.



---



这是我早些时候整理的计算机视觉和图像分割资料，还算比较全面：[计算机视觉笔记及资料整理（含图像分割、目标检测）.md](./notes/计算机视觉笔记及资料整理（含图像分割、目标检测）.md)

> 含图像分割入门基础、图像分割的模型发展及介绍、图像分割数据集等等。

[语义分割相关资料总结 - 知乎](https://zhuanlan.zhihu.com/p/41976717)



---

我在幕布上的记录：[04-图像语义分割(含FCN、UNet、SegNet、PSPNet、Deeplabv1&v2&v3等)](https://mubu.com/doc/1OEfnuDXAc)

- 常规的深度卷积神经网络 (如 AlexNet 和 VGG ) 并不适用于密集预测的任务。首先，这些模型包含许多用于减小输入特征的空间维度的层。结果，这些层最终产生缺乏清晰细节的高度抽象的特征矢量。第二，全连接层在计算过程中具有固定的输入规模和松散的空间信息。

- 作为一个例子，试想通过一系列的卷积来传递图像，而不是使用池化和全连接层。我们将每次卷积都设置成步长为 1，padding 为「SAME」。通过这种处理，每一次卷积都保留了输入的空间维度。我们可以堆叠很多这种卷积，并最终得到一个分割模型。用于密集预测的全卷积神经网络。**请注意，不存在池化层和全连接层。**

- 这个模型可以输出形状为 [W,H,C] 的概率张量，其中 W 和 H 代表的是宽度和高度，C 代表的是类别标签的个数。在第三个维度上应用最大化函数会得到形状为 [W,H,1] 的张量。然后，我们计算真实图像和我们的预测的每个像素之间的交叉熵。最终，我们对计算结果取平均值，并且使用反向传播算法训练网络。

- **然而**，这个方法存在一个问题。正如前面所提到的，使用步长为 1，padding 为「SAME」，保留了输入的维度。但是，那样做的后果就是模型会极其耗费内存，而且计算复杂度也是很大的。

- 为了缓解这个问题，分割网络通常会有三个主要的组成部分：卷积层、降采样层和上采样层。

  - **①图像语义分割模型的编码器-解码器结构。**
    - 在卷积神经网络中实现降采样的常用方式有两个：通过改变卷积步长或者常规的池化操作。一般而言，降采样的目标就是减少给定特征图的空间维度。因此，降采样可以让我们在执行更深的卷积运算时不用过多地考虑内存。然而，这样一来在计算的时候会损失一些特征。
    - 值得注意的是，这个架构的第一部分看上去类似于普通的分类深度卷积神经网络。不同的是，其中没有设置全连接层。
    - 有很多基于编码器—解码器结构的神经网络实现。**FCNs、SegNet，以及 UNet 是最流行的几个。**
  - **②膨胀卷积，抛弃了池化层。**

  ——*Form [语义分割网络DeepLab-v3的架构设计思想和TensorFlow实现 | 机器之心](https://www.jiqizhixin.com/articles/deeplab-v3)*

  





## (2) 语义分割的发展梳理

在学习完常见的语义分割模型后，可以看下面这些文章再梳理下。

### 语义分割发展和历史

（1）[10分钟看懂全卷积神经网络（FCN）：语义分割深度模型先驱 - TinyMind](https://www.tinymind.cn/articles/3815)

所有的发展都是漫长的技术积累，加上一些外界条件满足时就会产生质变。我们简单总结了图像分割的几个时期：

**2000年之前，数字图像处理时我们采用方法基于几类：阈值分割、区域分割、边缘分割、纹理特征、聚类等。**

**2000年到2010年期间， 主要方法有四类：基于图论、聚类、分类以及聚类和分类结合。**

**2010年至今，神经网络模型的崛起和深度学习的发展，主要涉及到几种模型：**

![](https://file.ai100.com.cn/files/sogou-articles/original/fb9f37e6-e925-4142-904b-3707fb984cd4/fb9f37e6-e925-4142-904b-3707fb984cd4)

截至到2017年底，我们已经分化出了数以百计的模型结构。当然，经过从技术和原理上考究，我们发现了一个特点，那就是当前最成功的图像分割深度学习技术都是基于一个共同的先驱：FCN（Fully Convolutional Network，全卷积神经网络）。

发展历程：

- 2014年 FCN 模型，主要贡献为在语义分割问题中推广使用端对端卷积神经网络，使用反卷积进行上采样

- 2015年 U-net 模型，构建了一套完整 的编码解码器

- 2015年 SegNet 模型，将最大池化转换为解码器来提高分辨率

- 2015年 Dilated Convolutions（空洞卷积），更广范围内提高了内容的聚合并不降低分辨率

- 2016年 DeepLab v1&v2

- 2016年 RefineNet 使用残差连接，降低了内存使用量，提高了模块间的特征融合

- 2016年 PSPNet 模型

- 2017年 Large Kernel Matters

- 2017年 DeepLab V3

以上几种模型可以按照语义分割模型的独有方法进行分类，如专门池化（PSPNet、DeepLab），编码器-解码器架构（SegNet、E-Net），多尺度处理（DeepLab）、条件随机场（CRFRNN）、空洞卷积（DiatedNet、DeepLab）和跳跃连接（FCN）。



（2）[深度学习（十九）——FCN, SegNet, DeconvNet, DeepLab, ENet, GCN](https://blog.csdn.net/antkillerfarm/article/details/79524417)

**前DL时代的语义分割：** 

Grab cut是微软剑桥研究院于2004年提出的著名交互式图像语义分割方法。与N-cut一样，grab cut同样也是基于图划分，不过grab cut是其改进版本，可以看作迭代式的语义分割算法。Grab cut利用了图像中的纹理（颜色）信息和边界（反差）信息，只要少量的用户交互操作即可得到比较好的前后背景分割结果。（注：这篇文章 [分割算法——可以分割一切目标（各种分割总结）](https://mp.weixin.qq.com/s/KcVKKsAyz-eVsyWR0Y812A) 也有提到 Grab cut 和 Grab cut）

在Grab cut中，RGB图像的前景和背景分别用一个高斯混合模型（Gaussian mixture model, GMM）来建模。两个GMM分别用以刻画某像素属于前景或背景的概率，每个GMM高斯部件（Gaussian component）个数一般设为k=5。接下来，利用吉布斯能量方程（Gibbs energy function）对整张图像进行全局刻画，而后迭代求取使得能量方程达到最优值的参数作为两个GMM的最优参数。GMM确定后，某像素属于前景或背景的概率就随之确定下来。

在与用户交互的过程中，Grab cut提供两种交互方式：一种以包围框（Bounding box）为辅助信息；另一种以涂写的线条（Scribbled line）作为辅助信息。以下图为例，用户在开始时提供一个包围框，grab cut默认的认为框中像素中包含主要物体／前景，此后经过迭代图划分求解，即可返回扣出的前景结果，可以发现即使是对于背景稍微复杂一些的图像，grab cut仍有不俗表现。

不过，在处理下图时，grab cut的分割效果则不能令人满意。此时，需要额外人为的提供更强的辅助信息：用红色线条／点标明背景区域，同时用白色线条标明前景区域。在此基础上，再次运行grab cut算法求取最优解即可得到较为满意的语义分割结果。Grab cut虽效果优良，但缺点也非常明显，一是仅能处理二类语义分割问题，二是需要人为干预而不能做到完全自动化。

不难看出，前DL时代的语义分割工作多是根据图像像素自身的低阶视觉信息（Low-level visual cues）来进行图像分割。由于这样的方法没有算法训练阶段，因此往往计算复杂度不高，但是在较困难的分割任务上（如果不提供人为的辅助信息），其分割效果并不能令人满意。



（3）[基于深度学习的语义分割简述及初探[译] - AIUAI](https://www.aiuai.cn/aifarm599.html)

语义分割是对图像的一种更精细的推断与理解，由粗到细为：

- 图像分类 - 初级的图片理解，其对整张图片进行整体理解.
- 目标定位与检测 - 不仅提供图像内的类别，还包括相对于物体类别的空间为位置信息.
- 语义分割 - 对每个图像像素进行密集预测，得到像素类别信息.

。。。

[1] 基于区域的语义分割

[2] 基于 FCN 的语义分割

[3] 弱监督语义分割



（4）[SLIC超像素分割详解（一）：简介](<https://blog.csdn.net/electech6/article/details/45509779>)

超像素概念是2003年Xiaofeng Ren提出和发展起来的图像分割技术，是指具有相似纹理、颜色、亮度等特征的相邻像素构成的有一定视觉意义的不规则像素块。它利用像素之间特征的相似性将像素分组,用少量的超像素代替大量的像素来表达图片特征,很大程度上降低了图像后处理的复杂度，所以通常作为分割算法的预处理步骤。已经广泛用于图像分割、姿势估计、目标跟踪、目标识别等计算机视觉应用。几种常见的超像素分割方法及其效果对比如下：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190330214949.png)

 Graph-based           NCut            Turbopixel          Quick-shift        Graph-cut a        Graph-cut b         SLIC

这里主要介绍的是SLIC（simple linear iterativeclustering），即简单的线性迭代聚类。它是2010年提出的一种思想简单、实现方便的算法，将彩色图像转化为CIELAB颜色空间和XY坐标下的5维特征向量，然后对5维特征向量构造距离度量标准，对图像像素进行局部聚类的过程。SLIC算法能生成紧凑、近似均匀的超像素，在运算速度，物体轮廓保持、超像素形状方面具有较高的综合评价，比较符合人们期望的分割效果。

### 语义分割模型发展

（1）[语义分割中的深度学习方法全解：从FCN、SegNet到各代DeepLab - 知乎](https://zhuanlan.zhihu.com/p/27794982)

该文要点：

1. FCN网络；
2. SegNet网络；
3. 空洞卷积(Dilated Convolutions)；
4. DeepLab (v1和v2)；
5. RefineNet；
6. PSPNet；
7. 大内核(Large Kernel Matters)；
8. DeepLab v3；

对于上面的每篇论文，将会分别指出主要贡献并进行解释，也贴出了这些结构在 VOC2012 数据集中的测试分值IOU。

（2）[主要语义分割网络模型概览[转] - AIUAI](https://www.aiuai.cn/aifarm602.html)

图像的语义分割是将输入图像中的每个像素分配一个语义类别，以得到像素化的密集分类。

虽然自 2007 年以来，语义分割/场景解析一直是计算机视觉社区的一部分，但与计算机视觉中的其他领域很相似，自 2014 年 Long 等人首次使用全卷积神经网络对自然图像进行端到端分割，语义分割才有了重大突破。

本文作者总结了 FCN、SegNet、U-Net、FC-Densenet E-Net 和 Link-Net、RefineNet、PSPNet、Mask-RCNN 以及一些半监督方法，如 DecoupledNet 和 GAN-SS，并为其中的一些网络提供了 PyTorch 实现. 在文章的最后一部分，作者总结了一些流行的数据集，并展示了一些网络训练的结果。



## (3) 图像分割衡量标准

像分割也是一项有意思的研究领域，它的目的是把图像中各种不同物体给用不同颜色分割出来，如下图所示，其平均精度（mIoU，即预测区域和实际区域交集除以预测区域和实际区域的并集），也从最开始的 FCN 模型（图像语义分割全连接网络，该论文获得计算机视觉顶会 CVPR2015 的最佳论文的）的 62.2%，到 DeepLab 框架的 72.7%，再到牛津大学的 CRF as RNN 的74.7%。该领域是一个仍在进展的领域，仍旧有很大的进步空间。

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190310160328.png)

。。。。。。

**精确度：**

图像分割中通常使用许多标准来衡量算法的精度。这些标准通常是像素精度及 IoU 的变种，以下我们将会介绍常用的几种逐像素标记的精度标准。为了便于解释，假设如下：共有 k+1 个类（从到，其中包含一个空类或背景），表示本属于类i但被预测为类 j 的像素数量。即，表示真正的数量，而则分别被解释为假正和假负，尽管两者都是假正与假负之和。

- Pixel Accuracy(**PA，像素精度**)：这是最简单的度量，为标记正确的像素占总像素的比例。 

- Mean Pixel Accuracy(**MPA，均像素精度**)：是 PA 的一种简单提升，计算每个类内被正确分类像素数的比例，之后求所有类的平均。 

- Mean Intersection over Union(**MIoU，均交并比**)：为语义分割的标准度量。其计算两个集合的交集和并集之比，在语义分割的问题中，这两个集合为真实值（ground truth）和预测值（predicted segmentation）。这个比例可以变形为正真数（intersection）比上真正、假负、假正（并集）之和。在每个类上计算 IoU，之后平均。 

- Frequency Weighted Intersection over Union(**FWIoU，频权交并比**)：为 MIoU 的一种提升，这种方法根据每个类出现的频率为其设置权重。 

  在以上所有的度量标准中，MIoU 由于其简洁、代表性强而成为最常用的度量标准，大多数研究人员都使用该标准报告其结果。

。。。。。。

**召回率、正确率：** 

针对预测样本而言，预测为正例的样本中真正正例的比例。 

预测为正的有两种： 

1. 正样本被预测为正 TP 
2. 负样本被预测为正 FP 

**所以精确率：precesion = TP/(TP+FP)    其中分母预测为正样本数量。**

针对原来的样本而言，表示样本中有多少正例被预测正确了（预测为正例的真是整理占所有真实正例的比例）： 

1. 原来的正样本被预测为正样本 TP 
2. 原来的正样本被预测为负样本 FN 

**所以召回率为：racall = TP/(TP+FN)      其中分母表示原来样本中的正样本数量。**

。。。。。。

参考：

- [图像分割的衡量指标详解 - qq_37274615的博客 - CSDN博客](https://blog.csdn.net/qq_37274615/article/details/78957962)
- [基于深度学习的图像语义分割技术概述之5.1度量标准](https://blog.csdn.net/u014593748/article/details/71698246)

(4) 评价指标
---

可以从以下几个指标评价某个分割算法的好坏：

- **mIoU**:这个指标是应用最多的，也是目前排名分割算法的依据。IoU就是每一个类别的交集与并集之比，而mIoU则是所有类别的平均IoU。**论文均使用这一指标比较。**
- **speed**:由于有些分割算法是针对实时语义分割设计的，所以速度也是一个很重要的评价指标，当然评价速度需要公平比较，包括使用的图像大小、电脑配置一致。
- 当然还有其他指标，如pixel accuracy、mean accuraccy等。

下面以一个简单的例子，说明怎么计算mIoU，由于分割也是分类问题，分类问题的指标一般使用**混淆矩阵**来求解。

mIoU：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190326162659.png)

其中(k+1)为类别数，pii表示TP，pij表示FN，pji表示 FP（i表示真实类别，j表示其他类别），则每一个类别的IoU可以看作，IoU=TP/(TP+FN+FP)。

以三个类别为例，如下是一个混淆矩阵：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190326162733.png)

对于类别1：TP=43，FN=7，FP=2；类别2：TP=45，FN=5，FP=6；类别3：TP=49，FN=1，FP=5.因此：IoU1=43/(43+2+7)=82.69%，IoU2=45/(45+5+6)=80.36%，IoU=49/(49+1+5)=89.09%，因此mIoU=84.05%.其实就是矩阵的每一行加每一列，再减去重复的TP。

**常用数据集：**

目前语义分割领域使用的数据集较多，这里主要介绍3个数据集，VOC2012/Cityscape/CamVid，所有数据集**均没有达到饱和**，仍有很大发展空间，无论在准确性还是实时性。

**忽视标签(ignore label)**

在介绍数据集之前，先来了解一下忽视标签的概念与作用。忽视标签从字面上理解，就是该标签不用于计算损失，也不用于计算精度。以一个三个类别的例子为例。

该例子，包含3个目标类别，1个背景类别，1个忽视类别。

标签可以设置为：0,1,2,3,4

其中，0代表背景，1-3代表目标类别，4代表忽视目标。

那么，目标类别数到底该设为多少呢？4还是5。

这里我看了很多地方，最简单的方式就是设置为4，这样训练过程中，4这个类别将不计算损失，同时我们也不需要进行预测argmax。

**（1）VOC2012**

原始VOC2012数据集：1464训练集，1449验证集，1456测试集。经过加强的数据集，包含10582训练集，1449验证集，1456测试集。**目前论文结果均使用后者进行训练。**该数据集特点包括：

- **数据集包含21类别目标，20类目标+1类背景**，分辨率大小不完全相同，为此训练过程为了能处理batch图像，因此需要将图像固定到某一大小，deeplab里面使用321大小。
- 该数据集中，label处理时发现并没有忽视标签，均在0-21之间，**因此不需要设置忽视标签。在deeplab版本里作者将**ignore_index设为255，标签里面本来就没有255，设不设置没关系.
- **每幅图像包含的目标类别很少，但每个目标的轮廓还是比较复杂，**因此，该数据集的分割精度没有想象中那么高。

**（2）CamVid**

训练集367，验证集101，测试集233。三个均有标注文件。有一下几个特点：

- **图像数量太少**，因此很少有人跑这个数据集，**只有少数轻量级分割网络会跑**，如ENet/Segnet/LinkNet/Bisenet等，现在能看到的最好精度也就60多mIOU。
- 原始数据集分辨率为960×720，而Segnet将其处理为480*360，后者也是使用得多，可能是因为Segnet将其处理后的文件夹上传的原因吧。
- **该数据集也存在忽视标签**，Segnet作者将其统一设置为11，类别数为11，标注为0-10，因此计算softmax损失时，需要设置ignore_index=11，或者在处理标签图片时，将mask[index==11] = -1，这样处理后不计算忽视标签的损失。
- 由于图像分辨率为480*360，且有11类目标，造成细小目标偏多，如行人，交通灯等。

**（3）Cityscape**

训练集2975，验证集500，测试集1525。训练集与验证集包括Fine annotations以及额外的19998个coarse annotations，可以于训练集一起训练。测试集标注没有公开。数据集主要特点包括：

。。。。。。



来源：[语义分割-从入门到放弃 - 知乎](https://zhuanlan.zhihu.com/p/48670341)
# 深度学习框架对比

## torch

模块名：Embeding layer

调用名：

``` python
torch.nn.Embedding(
    num_embeddings,
    embedding_dim,
    padding_idx=None,
    max_norm=None,
    norm_type=2,
    scale_grad_by_freq=False,
    sparse=False) 
```

参数：

``` python
num_embeddings--字典大小（vocab_size）
embedding_dim--嵌入维度（embed_dim）
padding_idx--如果提供的话，输出遇到此下标时用零填充，
max_norm--如果提供的话，会重新归一化词嵌入，使它们的范数小于提供的值
norm_type--对于max_norm选项计算p范数时的p,
scale_grad_by_freq--如果提供的话，会根据字典中单词频率缩放梯度,
sparse=False
```

预训练：

``` python
nn.Embedding.from_pretrained(weight)
```

实例：

``` python
import torch
embedding = torch.nn.Embedding(10, 3)
input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
embedding(input)
result:
   tensor([[[-0.0251, -1.6902,  0.7172],
            [-0.6431,  0.0748,  0.6969],
            [ 1.4970,  1.3448, -0.9685],
            [-0.3677, -2.7265, -0.1685]],

           [[ 1.4970,  1.3448, -0.9685],
            [ 0.4362, -0.4004,  0.9400],
            [-0.6431,  0.0748,  0.6969],
            [ 0.9124, -2.3616,  1.1151]]])
```



## tensorflow

模块名：Embeding layer

调用名：

``` python
tf.nn.embedding_lookup(
    params,
    ids,
    partition_strategy='mod',
    name=None,
    max_norm=None)
```

参数：

``` xml
params--输入类型（vocab_size,embed_dim）
ids--输入数据 input 数据
partition_strategy--指定切分策略的字符串,在 len(params) > 1 的情况下使用.,
name--名字,
max_norm--如果提供该参数,embedding 值将被 L2-normalize 为 max_norm 的值
```

预训练：

``` python
如果使用预训练直接把params设置为预训练向量
```



## keras

模块名：Embeding layer

调用名：

``` python
keras.layers.Embedding( input_dim,output_dim,
    embeddings_initializer='uniform',embeddings_regularizer=None,
    activity_regularizer=None,embeddings_constraint=None,
    mask_zero=False,
    input_length=None）
```

参数：

``` xml
input_dim, output_dim,
embeddings_initializer='uniform', embeddings_regularizer=None,
activity_regularizer=None, embeddings_constraint=None,
mask_zero=False,
input_length=None
```

*——from：某个群文件*




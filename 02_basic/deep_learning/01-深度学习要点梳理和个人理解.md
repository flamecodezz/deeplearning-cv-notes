# 深度学习要点梳理和个人理解

这里记录下对深度学习某些知识和内容的探究、梳理，以及个人理解。



## 写在前面

:notes: 有一些学习要点的笔记和梳理我有随记在「幕布」和「有道云笔记」平台，挺杂乱的，但值得看一看：

- [01-机器学习&深度学习要点小梳理（含资料推荐）](https://mubu.com/doc/2E8oghDU78)
- [02-卷积神经网络CNN探究(含反卷积、转置卷积、空洞卷积、上采样、下采样)](https://mubu.com/doc/3LbSzN4z-8)
- [03-CNN网络模型演进(LetNet、AleNet、VGGNet、NIN、GoogleNet、ResNet等)](https://mubu.com/doc/2BFlc9r-B8)
- [04-图像语义分割(含FCN、UNet、SegNet、PSPNet、Deeplabv1&v2&v3等)](https://mubu.com/doc/1OEfnuDXAc)
- [数据预处理：中心化（又叫零均值化）和标准化（又叫归一化）](https://mubu.com/doc/2qC5u7sGw8)
- [对softmax，softmax loss和cross entropy的理解](https://mubu.com/doc/T0NtYmnFc)
- ……

里面的内容有的会继续补充，另外，有新作的要点和内容，我会以分享链接的形式继续更新在上面，不过以上的分享链接也有可能指不定哪天被我取消了，不过关注该仓库即可，日后有时间整理成文至此。

注：对于在「幕布」上作的内容，有的我有导出成了 HTML 文件，可以在该文目录下的 html 文件夹下找到，但在 GitHub 上直接打开 HTML 文件只能看到源代码，看不到显示效果，如果想要看到正常文件内容，可以打开  [htmlpreview](https://htmlpreview.github.io/)，然后复制 HTML 文件的地址链接过去即可看到内容。

---



### 关于反卷积、转置卷积、带孔卷积、Pooling层提高感受野等的理解

（1）

- [dilated conv带孔卷积、pooling层提高感受野 反卷积 的理解 - CSDN博客](https://blog.csdn.net/jiachen0212/article/details/78548667)  [荐，看完你会明白的~]

  > pooling 为什么可以提高感受野？得这样理解：首先它第一个作用是降低 feature map 的尺寸，减少需要训练的参数；其次，因为有缩小的作用，所以之前的 4 个像素点，现在压缩成 1 个。那么，相当于我透过这 1 个点，就可以看到前面的 4 个点，这不就是把当前 map 的感受野一下子放大了嘛。所以就有以下结论：pooling 降维减少参数量，并且增大感受野。
  >
  > 。。。
  >
  > 因此，提出了 dilated conv 带孔卷积，解决保持感受野和保护图像尺寸间的互斥问题。
  >
  > 需要好好理解的地方是：带孔卷积并不是卷积核里带孔，而是在卷积的时候，跳着的去卷积 map（比如 dilated＝2 的孔卷积，就是隔一个像素点，“卷”一下，这就相当于把卷积核给放大了（3x3 的核变成 7x7的核，多出位置的 weights 给 0 就是。）这样就使得 3x3 的卷积核也能达到 7x7 卷积核的感受野（因为它在原 map 上，确实 9 个点就跨越覆盖到了传统 conv 的49个像素点），也就扩大了感受野了，使得少一些 pooling 层也没关系。。。

同时推荐看下下面的文章：

- [对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解 | eamlife's blog](<https://lonepatient.top/2018/02/27/understand-convolution.html>)
- [对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解 - CSDN](<https://blog.csdn.net/Chaolei3/article/details/79374563>)

（2）

有个问题，知乎上关于空洞卷积有个[回答](<https://www.zhihu.com/question/54149221/answer/192025860>)：

> ![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190504130639.png)
>
> “(a)图对应3x3的1-dilated conv，和普通的卷积操作一样，(b)图对应3x3的2-dilated conv，实际的卷积kernel size还是3x3，但是空洞为1，也就是对于一个7x7的图像patch，只有9个红色的点和3x3的kernel发生卷积操作，其余的点略过。也可以理解为kernel的size为7x7，但是只有图中的9个点的权重不为0，其余都为0。 可以看到虽然kernel size只有3x3，但是这个卷积的感受野已经增大到了7x7（如果考虑到这个2-dilated conv的前一层是一个1-dilated conv的话，那么每个红点就是1-dilated的卷积输出，所以感受野为3x3，所以1-dilated和2-dilated合起来就能达到7x7的conv）,(c)图是4-dilated conv操作，同理跟在两个1-dilated和2-dilated conv的后面，能达到15x15的感受野。对比传统的conv操作，3层3x3的卷积加起来，stride为1的话，只能达到(kernel-1)*layer+1=7的感受野，也就是和层数layer成线性关系，而dilated conv的感受野是指数级的增长。”

其中关于空洞卷积感受野的理解，比如 3x3 的空洞卷积怎么相当于 7x7 的感受野？后来看到某篇博客的下面评论也有提到这个：<https://blog.csdn.net/Quincuntial/article/details/78743033#commentsedit>

> “写得很好，不过有一个问题。 对于2-dilated convolution，卷积核的感受野是5x5。只有算上前面的3x3的卷积，在这一层的感受野才能达到7x7。正因如此，只有加上前面1-dilated convoluton和2-dilated convoluton，第三个4-dilated convolution的感受野才能达到225x255. 望作者更正，指明不是卷积核的感受野，而是加上前面的卷积核总共的感受野。”

**通过与别人交流，我觉得大概是这么理解的，举例子：**比如对于 7x7 的图像输入，如果先用 1-dilated conv（相当于用 3x3 的 filter），步长为 1 ---> 5x5的feature map --->再用 2-dilated  conv（相当用 5x5的filter）---> 1x1的feature map；但如果感受野就是 7x7，对于 7x7 的图像输入，正常卷积，最后也是得到 1x1 的feature map。如此才会看到知乎那个回答说“**1-dilated和2-dilated合起来就能达到7x7的conv**”。

最后：

> 带孔卷积的感受野可以由这个公式计算得到：![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190504130422.png)；其中 i+1 表示 dilated rate。 
> 比如上图中（a），dilated=1，F(dilated) = 3×3；图（b）中，dilated=2，F(dilated)=7×7；图（c）中，dilated=4， F(dilated)=15×15。 



## 1. 深度学习之笔记梳理

### (1) 我的理解：神经网络参数改变过程？

见：[我的理解：神经网络参数改变过程？.md](./我的理解：神经网络参数改变过程？.md)

### (2)  什么是端对端训练？

《深度学习之美：AI时代的数据处理与最佳实践》：

> P9：“end to end”（端对对）说的是，输入的是原始数据（始端），然后输出的直接就是最终目标（末端），中间过程不可知。

### (3) 机器学习中训练集、验证集、测试集如何划分

1.传统的机器学习领域中，由于收集到的数据量往往不多，比较小，所以需要将收集到的数据分为三类：训练集、验证集、测试集。也有人分为两类，就是不需要测试集。

比例根据经验不同而不同，这里给出一个例子，如果是三类，可能是训练集：验证集：测试集=6:2:2；如果是两类，可能是训练集：验证集=7:3。因为数据量不多，所以验证集和测试集需要占的数据比例比较多。

2.在大数据时代的机器学习或者深度学习领域中，如果还是按照传统的数据划分方式不是十分合理，因为测试集和验证集用于评估模型和选择模型，所需要的数据量和传统的数据量差不多，但是由于收集到的数据远远大于传统机器学习时代的数据量，所以占的比例也就要缩小。比如我们拥有 1000000，这么多的数据，训练集：验证集：测试集=98:1:1。如果是两类，也就是相同的道理。

注意：有些人在把数据分类的时候是没有测试集数据，这样并不是十分合理，有测试集比较放心，建议把数据分类最好有这个数据集，也就是分为三类数据。



## 2. 深度学习之数学基础

### (1) 什么是标准差和方差？

推荐阅读：[标准差和方差](https://www.shuxuele.com/data/standard-deviation.html)

总结：当数据比较分散时，标准差也比较大，即**数据越分数，标准差也越大**。另外，试着体会这句话：**“差”的意思是离正常有多远。**

### (2) 什么是中心化和标准化？

（1）中心化

中心化（又叫零均值化）：是指变量减去它的均值。其实就是一个平移的过程，平移后所有数据的中心是（0，0）。

``` xml
零均值化就是一组数据，其中每一个都减去这组的平均值。
例如，对1、2、3、4、5零均值化，
先算出其均值为3，然后每一个数都减去3，
得到-2、-1、0、1、2
就实现了零均值化
```

（2）标准化

标准化（又叫归一化）： 是指数值减去均值，再除以标准差。

Q：意义 - 为何需要这些预处理？？？

> 在一些实际问题中，我们得到的样本数据都是多个维度的，即一个样本是用多个特征来表征的。比如在预测房价的问题中，影响房价的因素（特征）有房子面积、卧室数量等，很显然，这些特征的量纲和数值得量级都是不一样的，在预测房价时，如果直接使用原始的数据值，那么他们对房价的影响程度将是不一样的，而通过标准化处理，可以使得不同的特征具有**相同的尺度（Scale）**。简言之，当原始数据不同维度上的**特征的尺度（单位）不一致**时，需要**标准化**步骤对数据进行预处理。

下图中以二维数据为例：左图表示的是原始数据；中间的是中心化后的数据，数据被移动大原点周围；右图将中心化后的数据除以标准差，得到为标准化的数据，可以看出每个维度上的尺度是一致的（红色线段的长度表示尺度）。

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190522114512.png)

其实，在不同的问题中，中心化和标准化有着不同的意义，比如在训练神经网络的过程中，通过将数据标准化，能够加速权重参数的收敛。对数据进行中心化预处理，这样做的目的是要增加基向量的正交性。

标准化（归一化）两个优点：

  1）归一化后加快了梯度下降求最优解的速度；

  2）归一化有可能提高精度。

补充：[数据标准化/归一化normalization](<https://blog.csdn.net/pipisorry/article/details/52247379>)

> 数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。
>
> 目前数据标准化方法有多种，归结起来可以分为直线型方法(如极值法、标准差法)、折线型方法(如三折线法)、曲线型方法(如半正态性分布)。不同的标准化方法，对系统的评价结果会产生不同的影响，然而不幸的是，在数据标准化方法的选择上，还没有通用的法则可以遵循。
>
> 其中最典型的就是数据的**归一化处理**，即将数据统一映射到[0,1]区间上。
>
> 最常见的归一化方法是 min-max标准化 和 z-score 标准化。
>
> min-max标准化：![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190522115752.png)
>
> z-score标准化（0-1标准化）方法：![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190522115819.png)

补充：什么是量纲？ - [量纲是什么意思? - 百度知道](<https://zhidao.baidu.com/question/79891293.html?qbl=relate_question_2>)

参考：

- [中心化（又叫零均值化）和标准化（又叫归一化）](https://blog.csdn.net/GoodShot/article/details/80373372)
- [数据预处理之中心化（零均值化）与标准化（归一化）](https://www.cnblogs.com/wangqiang9/p/9285594.html)



### (3) 什么是正太分布（又名高斯分布）？

正态分布（Normal distribution）又名高斯分布（Gaussian distribution），是一个在数学、物理及工程等领域都非常重要的概率分布，在统计学的许多方面有着重大的影响力。——from：<https://blog.csdn.net/renwudao24/article/details/44463489>

推荐阅读：[正态分布为什么常见？ - 阮一峰的网络日志](http://www.ruanyifeng.com/blog/2017/08/normal-distribution.html)

统计学里面，正态分布（normal distribution）最常见。男女身高、寿命、血压、考试成绩、测量误差等等，都属于正态分布。

。。。。。。

比如，财富的分布就是不对称的，富人的有钱程度（可能比平均值高出上万倍），远远超出穷人的贫穷程度（平均值的十分之一就是赤贫了），即财富分布曲线有右侧的长尾。相比来说，身高的差异就小得多，最高和最矮的人与平均身高的差距，都在30%多。

这是为什么呢，财富明明也受到多种因素的影响，怎么就不是正态分布呢？

原来，正态分布只适合各种因素累加的情况，如果这些因素不是彼此独立的，会互相加强影响，那么就不是正态分布了。一个人是否能够挣大钱，由多种因素决定：

- 家庭
- 教育
- 运气
- 工作
- ...

这些因素都不是独立的，会彼此加强。如果出生在上层家庭，那么你就有更大的机会接受良好的教育、找到高薪的工作、遇见好机会，反之亦然。也就是说，这不是 1 + 1 = 2 的效果，而是 1 + 1 > 2。

统计学家发现，如果各种因素对结果的影响不是相加，而是相乘，那么最终结果不是正态分布，而是[对数正态分布](https://baike.baidu.com/item/%E5%AF%B9%E6%95%B0%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)（log normal distribution），即`x`的对数值`log(x)`满足正态分布。

这就是说，财富的对数值满足正态分布。如果平均财富是10,000元，那么1000元～10,000元之间的穷人（比平均值低一个数量级，宽度为9000）与10,000元~100,000元之间的富人（比平均值高一个数量级，宽度为90,000）人数一样多。因此，财富曲线左侧的范围比较窄，右侧出现长尾。

推荐阅读：[怎样用通俗易懂的文字解释正态分布及其意义？ - 知乎](<https://www.zhihu.com/question/56891433>)

### (4) 什么是概率分布？

参考：

- [应该如何理解概率分布函数和概率密度函数？ - 简书](<https://www.jianshu.com/p/b570b1ba92bb>)
- [读了本文，你就懂了概率分布 - 知乎](<https://zhuanlan.zhihu.com/p/26810566>)
- [概率分布 - MBA智库百科](<https://wiki.mbalib.com/wiki/%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83>)

### (5) 什么是鞍点？

关于「鞍点」的定义：

（1）鞍点附近的某些点比鞍点有更大的代价，而其他点则有更小的代价。

（2）一个不是局部极值点的驻点称为鞍点。

Notes：

- 驻点：一阶导数为0；
- 拐点：二阶导数为0。

Example：

（1）单变量函数：

鞍点处的一阶导为0，二阶导换正负号。 

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190311103632.gif)

（2）多变量函数：

鞍点处，在某些方向上是峰顶，在其他方向上是谷底。 

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190311111410.jpg)

**Note：** 

- 在**高维空间**中，**局部最优**很**罕见**，**鞍点**很**常见** (在低维空间中则相反)；
- 对于模型而言，它**并不知道**自己究竟走到的是驻点还是局部最优点。所幸的是我们常用**带动量的 SGD**。

参考：[深度学习: 鞍点 - Online Note - CSDN博客](https://blog.csdn.net/JNingWei/article/details/79801699)

### (6) 凸优化和非凸优化

为什么要求是凸函数呢？因为如果是下图这样的函数，则无法获得全局最优解。

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190408123740.png)

为什么要求是凸集呢？因为如果可行域不是凸集，也会导致局部最优：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190408123814.png)

之所以要区分凸优化问题和非凸的问题原因在于凸优化问题中局部最优解同时也是全局最优解，这个特性使凸优化问题在一定意义上更易于解决，而一般的非凸最优化问题相比之下更难解决。

非凸优化问题如何转化为凸优化问题的方法：

- 1）修改目标函数，使之转化为凸函数

- 2）抛弃一些约束条件，使新的可行域为凸集并且包含原可行域

参考：[凸优化和非凸优化 - CSDN](<https://blog.csdn.net/kebu12345678/article/details/54926287>)

## 3. 深度学习之CV有关问题

### (1) top5错误率

参考：[什么是图像分类的Top-5错误率？ - 知乎](https://www.zhihu.com/question/36463511)

- 回答1：imagenet图像通常有1000个可能的类别，对每幅图像你可以猜5次结果(即同时预测5个类别标签)，当其中有任何一次预测对了，结果都算对，当5次全都错了的时候，才算预测错误，这时候的分类错误率就叫top5错误率。

- 回答2：翻译一下和Yong Pan答案是一样的，top1就是你预测的label取最后概率向量里面最大的那一个作为预测结果，你的预测结果中概率最大的那个类必须是正确类别才算预测正确。而top5就是最后概率向量最大的前五名中出现了正确概率即为预测正确。

### (2) 条件随机场CRF在语义分割上的应用

对于每个像素位置 i 具有隐变量 (这里隐变量就是像素的真实类别标签，如果预测结果有 21 类，则 (i∈1,2,..,21)，还有对应的观测值 yi (即像素点对应的颜色值)。以像素为节点，像素与像素间的关系作为边，构成了一个“条件随机场”（ Conditional Random Field，简称 CRF）。通过观测变量 yi 来推测像素位置 i 对应的类别标签 xi。条件随机场示意图如下：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190303194042.png)

参考：[语义分割论文-DeepLab系列](http://t.cn/E2z2Bs6)

很多以深度学习为框架的图像语义分割系统都使用了一种叫做 “条件随机场”（ Conditional Random Field，简称 **CRF**）的技术作为输出结果的优化后处理手段。其实类似技术种类较多，比如还有马尔科夫随机场 (MRF) 和高斯条件随机场 (G-CRF) 用的也比较多，但原理都较为类似。

简单来介绍一下 “条件随机场” 的概念。

FCN 是像素到像素的影射，所以最终输出的图片上每一个像素都是标注了分类的，将这些分类简单地看成是不同的变量，每个像素都和其他像素之间建立一种连接，连接就是相互间的关系。于是就会得到一个 “完全图”：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190303194723.png)



上图是以 4x6 大小的图像像素阵列表示的简易版。那么在全链接的 CRF 模型中，有一个对应的能量函数：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190303194936.png)

嗯，不要问我这个公式里各种符号是啥，我看不懂。但是我知道这个公式是干嘛滴：

其中等号右边第一个一元项，表示像素对应的语义类别，其类别可以由 FCN 或者其他语义分割模型的预测结果得到；而第二项为二元项，二元项可将像素之间的语义联系 / 关系考虑进去。

这么说太抽象，举个简单的例子，“天空”和 “鸟” 这样的像素在物理空间是相邻的概率，应该要比 “天空” 和 “鱼” 这样像素相邻的概率大，那么天空的边缘就更应该判断为鸟而不是鱼（从概率的角度）。

通过对这个能量函数优化求解，把明显不符合事实识别判断剔除，替换成合理的解释，得到对 FCN 的图像语义预测结果的优化，生成最终的语义分割结果。

*——from：[十分钟看懂图像语义分割技术 | 雷锋网](https://www.leiphone.com/news/201705/YbRHBVIjhqVBP0X5.html)*





---

*update：2019-03-03* 